{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff55ebb5-6e57-451f-90da-4a8356274d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal path: [(0, 0), (1, 0), (2, 0), (2, 1), (2, 2), (3, 2), (4, 2), (4, 3), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.9, exploration_prob=0.2):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_prob = exploration_prob\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "    def choose_action(self, state, explore=True):\n",
    "        if explore and np.random.rand() < self.exploration_prob:\n",
    "            # Explore: choose a random action\n",
    "            return np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q value\n",
    "            return np.argmax(self.q_table[state, :])\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state, :])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action]\n",
    "        td_error = td_target - self.q_table[state, action]\n",
    "        self.q_table[state, action] += self.learning_rate * td_error\n",
    "\n",
    "def run_maze():\n",
    "    # Define the maze as a 2D array\n",
    "    maze = np.array([\n",
    "        [0, 1, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 1, 1],\n",
    "        [0, 1, 0, 0, 0]\n",
    "    ])\n",
    "\n",
    "    n_states = np.prod(maze.shape)\n",
    "    n_actions = 4  # 0: up, 1: down, 2: left, 3: right\n",
    "\n",
    "    agent = QLearningAgent(n_states, n_actions)\n",
    "\n",
    "    # Convert 2D coordinates to a single index\n",
    "    def state_to_index(state):\n",
    "        return state[0] * maze.shape[1] + state[1]\n",
    "\n",
    "    # Convert single index to 2D coordinates\n",
    "    def index_to_state(index):\n",
    "        return divmod(index, maze.shape[1])\n",
    "    \n",
    "    # Check if the action is valid (i.e., not hitting a wall)\n",
    "    def check_action(action, state):\n",
    "        if action == 0 and state[0] > 0 and maze[state[0] - 1, state[1]] != 1:\n",
    "            return (state[0] - 1, state[1])\n",
    "        if action == 1 and state[0] < maze.shape[0] - 1 and maze[state[0] + 1, state[1]] != 1:\n",
    "            return (state[0] + 1, state[1])\n",
    "        if action == 2 and state[1] > 0 and maze[state[0], state[1] - 1] != 1:\n",
    "            return (state[0], state[1] - 1)\n",
    "        if action == 3 and state[1] < maze.shape[1] - 1 and maze[state[0], state[1] + 1] != 1:\n",
    "            return (state[0], state[1] + 1)\n",
    "        # Stay in the current state if the action is invalid\n",
    "        return state\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 1000\n",
    "    for epoch in range(epochs):\n",
    "        state = (0, 0)  # Starting position\n",
    "        goal = (4, 4)   # Goal position\n",
    "\n",
    "        while state != goal:\n",
    "            index = state_to_index(state)\n",
    "            action = agent.choose_action(index)\n",
    "            next_state = check_action(action, state)\n",
    "\n",
    "            # Calculate reward (negative for each step, additional penalty for hitting walls)\n",
    "            reward = -1 if state != goal else 0\n",
    "            if state == next_state:\n",
    "                reward -= 5  # Additional penalty for hitting walls\n",
    "\n",
    "            # Update Q value\n",
    "            agent.update_q_value(index, action, reward, state_to_index(next_state))\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "    # Testing\n",
    "    state = (0, 0)\n",
    "    path = [state]\n",
    "    while state != goal:\n",
    "        index = state_to_index(state)\n",
    "        action = agent.choose_action(index, explore=False)  # Exploit only, no exploration\n",
    "        next_state = check_action(action, state)\n",
    "\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "\n",
    "    print(\"Optimal path:\", path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_maze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
